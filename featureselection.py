# -*- coding: utf-8 -*-
"""FeatureSelection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QBiFrergtyF8KcTznUlnTV8cvOHTPm5q
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import VarianceThreshold
from sklearn.model_selection import cross_val_score, train_test_split

from sklearn.datasets import make_classification
x_data_generated, y_data_generated = make_classification(scale=1)

cross_val_score(LogisticRegression(), x_data_generated,y_data_generated , scoring='accuracy' ).mean()

df = pd.DataFrame(x_data_generated)
df.columns =['0','1', '2', '3','4', '5', '6', '7','8', '9', '10', '11','12', '13', '14', '15','16', '17', '18', '19']
df.insert (loc= 0 , column='y', value=y_data_generated)
df.columns = df.columns.str.replace(' ', '')
df.head()

X = df.drop(columns = 'y')
y = df['y']
X_train,X_test,y_test,y_train = train_test_split(X,y,test_size = 0.2, random_state = 21)
X.shape

df.info()

y.value_counts

corr = df.corr()
plt.figure(figsize=(14,10))
sns.heatmap(corr, annot=True)
plt.show()
#Неважные признаки - 0,1,2,3,4,5,6,7,8,10,11,15,17
#Важные признаки - 9,14

vt = VarianceThreshold(threshold = 1)
X_new = vt.fit_transform(X)
X_new.shape

cross_val_score(LogisticRegression(), X_new,y_data_generated , scoring='accuracy' ).mean()
#Ничего не изменилось значит избавились от лишних признаков верно

from sklearn.feature_selection import SelectKBest, f_classif
X_new2 = SelectKBest(f_classif, k=5).fit_transform(X, y)
X_new2.shape

cross_val_score(LogisticRegression(), X_new2,y, scoring='accuracy' ).mean()
#Стало лучше

from sklearn.feature_selection import SelectFromModel
sfm = SelectFromModel(estimator=LogisticRegression().fit(X,y), threshold = 'l1')

data = pd.DataFrame(data={'score':sfm.estimator.coef_[0]},
             index=X.columns).sort_values(by='score',ascending=False)
data.head()
#4,14,2,5
#Разница между 5 и 6 уже незначительна, оставлю первые 4 признака

newXsfm = df[['4', '14', '2', '5']]

cross_val_score(LogisticRegression(), newXsfm, y, scoring='accuracy').mean()
#Без изменений

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(random_state = 21)
rfc.fit(X,y)



rfc.score(X,y)

imp = pd.Series(rfc.feature_importances_)
imp.index = pd.Series(X.columns)
imp = imp.sort_values(ascending = False)
imp

plt.title('5 важных признаков')
plt.xlabel('Признак')
plt.ylabel('Gini')
plt.bar(imp.head().index,imp.head())
plt.show()

newXrfc = df[['4', '14', '11', '12', '9']]
cross_val_score(LogisticRegression(), newXrfc, y, scoring='accuracy').mean()
#Без изменений

from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=2)
sfs = SequentialFeatureSelector(knn,n_features_to_select = 5)
sfs.fit(X,y)

sfs.get_support()

newXsfs = sfs.transform(X)

cross_val_score(LogisticRegression(), newXsfs, y, scoring='accuracy').mean()
#Стало даже чутка лучше

vivod = pd.DataFrame(columns=["способ выбора признаков", "кол-во признаков", "средняя точность модели"])

vivod.loc[len(vivod)]=['без выбора', 20, 0.839]
vivod.loc[len(vivod)]=['удаление корреляций и низковариативных', 11, 0.86]
vivod.loc[len(vivod)]=['SelectKBest', 5, 0.869]
vivod.loc[len(vivod)]=['SelectFromModel', 4, 0.89]
vivod.loc[len(vivod)]=['RandomForestClassifier', 5, 0.869]
vivod.loc[len(vivod)]=['SequentialFeatureSelector', 5, 0.87]

vivod.head(6)